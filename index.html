
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="cvpr, workshop, computer vision, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/ico/favicon.png">



  <title>ScanNet Indoor Scene Understanding Challenge</title>
  <meta name="description" content="Website for the ScanNet Indoor Scene Understanding CVPR 2021 Workshop ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="ScanNet Indoor Scene Understanding Challenge"/>
  <meta property="og:url" content="http://www.scan-net.org/cvpr2019workshop"/>
  <meta property="og:description" content="Website for the ScanNet Indoor Scene Understanding CVPR 2021 Workshop ---"/>
  <meta property="og:site_name" content="ScanNet Indoor Scene Understanding Challenge"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="ScanNet Indoor Scene Understanding Challenge"/>
  <meta name="twitter:image" content="http://www.scan-net.org/cvpr2019workshop/static/img/splash.jpg">
  <meta name="twitter:url" content="http://www.scan-net.org/cvpr2019workshop"/>
  <meta name="twitter:description" content="Website for the ScanNet Indoor Scene Understanding CVPR 2021 Workshop ---"/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="static/css/main.css" media="screen,projection">

  <script type="text/javascript" src="static/js/jquery.min.js"></script>
  <script type="text/javascript" src="static/js/bootstrap.min.js"></script>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>3rd ScanNet Indoor Scene Understanding Challenge</h1></center>
    <center><h2>CVPR 2021 Workshop</h2></center>
    <center>June 19, 2021</center>
  </div>
</div>

<hr />

<!-- <br>
  <center>
  <h1 style="color:blue"><a href="https://youtu.be/S7wm806JQ1M">Join Us Here: Live Stream</a></h1>
  </center>
<br> -->

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="/cvpr2021workshop/static/img/splash.jpg" />
  </div>
</div>

<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      3D scene understanding for indoor environments is becoming an increasingly important area.
      Application domains such as augmented and virtual reality, computational photography, interior design, and autonomous mobile robots all require a deep understanding of 3D interior spaces, the semantics of objects that are present, and their relative configurations in 3D space.
    </p>
    <p>
      We present the first comprehensive challenge for 3D scene understanding of entire rooms at the object instance-level with 5 tasks based on the ScanNet dataset.
      The ScanNet dataset is a large-scale semantically annotated dataset of 3D mesh reconstructions of interior spaces (approx. 1500 rooms and 2.5 million RGB-D frames).
      It is used by more than 480 research groups to develop and benchmark state-of-the-art approaches in semantic scene understanding.
      A key goal of this challenge is to compare state-of-the-art approaches operating on image data (including RGB-D) with approaches operating directly on 3D data (point cloud, or surface mesh representations).
      Additionally, we pose both object category label prediction (commonly referred to as semantic segmentation), and instance-level object recognition (object instance prediction and category label prediction).
      We propose five tasks that cover this space:
    </p>
    <ul>
      <li>
        <strong>2D semantic label prediction</strong>: prediction of object category labels from 2D image representation
      </li>
      <li>
        <strong>2D semantic instance prediction</strong>: prediction of object instance and category labels from 2D image representation
      </li>
      <li>
        <strong>3D semantic label prediction</strong>: prediction of object category labels from 3D representation
      </li>
      <li>
        <strong>3D semantic instance prediction</strong>: prediction of object instance and category labels from 3D representation
      </li>
      <li>
        <strong>Scene type classification</strong>: classification of entire 3D room into a scene type
      </li>
    </ul>
      <h3 style="color:orange;font-weight:800">New This Year - Data Efficient Challenge!</h3>
    <p>In the data efficient challenge, training is conducted on  Limited Scene Reconstructions (LR) or Limited Scene Annotations (LA), for the tasks of 3D Semantic Segmentation, Instance Segmentation and Object Detection. 
    </p>
    <p>
      For each task, challenge participants are provided with prepared training, validation, and test datasets, and automated evaluation scripts.
      In addition to the public train-val-test split, benchmarking is done on a hidden test set whose raw data can be downloaded without annotations; in order to participate in the benchmark, the predictions on the hidden test set are uploaded to the evaluation server, where they are evaluated.
      Submission is restricted to submissions every two weeks to avoid finetuning on the test dataset.
      See more details at <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/documentation">http://kaldir.vc.in.tum.de/scannet_benchmark/documentation</a> if you would like to participate in the challenge.
      The evaluation server leaderboard is live at <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/">http://kaldir.vc.in.tum.de/scannet_benchmark/</a>.
      See the new data efficient <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/data_efficient/documentation">documentation</a> and <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/data_efficient">leaderboard</a>!.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row" id="tasks">
  <div class="col-md-6 text-center">
    <img src="/cvpr2021workshop/static/img/semantic_label_2d.jpg" />
    <p>2D semantic label prediction</p>
  </div>
  <div class="col-md-6 text-center">
    <img src="/cvpr2021workshop/static/img/semantic_instance_2d.jpg" />
    <p>2D semantic instance prediction</p>
  </div>
  <div class="col-md-6 text-center">
    <img src="/cvpr2021workshop/static/img/semantic_label_3d.jpg" />
    <p>3D semantic label prediction</p>
  </div>
  <div class="col-md-6 text-center">
    <img src="/cvpr2021workshop/static/img/semantic_instance_3d.jpg" />
    <p>3D semantic instance prediction</p>
  </div>
  <!-- <div class="col-md-4">
    <p>&nbsp;</p>
  </div> -->
  <!-- <div class="col-md-4">
    <img src="/cvpr2021workshop/static/img/scene_type_classification.jpg">
    <p>Scene type classification</p>
  </div> -->
</div>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr><td>TBD</td><td>TBD</td></tr>
        <!-- <tr>
          <td>Poster Submission Deadline</td>
          <td>May 20 2020</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 25 2020</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>June 19 2020</td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div>
<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Posters</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To submit a poster to the workshop, please email the poster as .pdf file to scannet@googlegroups.com.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr><td>TBD</td><td>TBD</td></tr>
        <!-- <tr>
          <td>Welcome and Introduction</td>
          <td>1:50pm - 2:00pm</td>
        </tr>
        <tr>
          <td>Implicit Neural Representations: From Objects to 3D Scenes (Andreas Geiger)</td>
          <td>2:00pm - 2:30pm</td>
        </tr>
        <tr>
          <td>Winner Talk: KPConv (Hugues Thomas) </td>
          <td>2:30pm - 2:40pm</td>
        </tr>
        <tr>
          <td>Winner Talk: SparseConvNet (Benjamin Graham) </td>
          <td>2:40pm - 2:50pm</td>
        </tr>
        <tr>
          <td>Winner Talk: PointGroup (Li Jiang) </td>
          <td>2:50pm - 3:00pm</td>
        </tr>
        <tr>
          <td>Break </td>
          <td>3:00pm - 3:15pm</td>
        </tr>
        <tr>
          <td>Winner Talk: Sparse 3D Perception (Chris Choy) </td>
          <td>3:15pm - 3:25pm</td>
        </tr>
        <tr>
          <td>Winner Talk: 3D-MPA (Francis Engelmann) </td>
          <td>3:25pm - 3:35pm</td>
        </tr>
        <tr>
          <td>Winner Talk: OccuSeg (Tian Zheng) </td>
          <td>3:35pm - 3:45pm</td>
        </tr>
        <tr>
          <td>Break </td>
          <td>3:45pm - 4:00pm</td>
        </tr>
        <tr>
          <td>CVPR is a Contemporary Art Exhibition (Yasutaka Furukawa)</td>
          <td>4:00pm - 4:30pm</td>
        </tr>
        <tr>
          <td>Semantic Scene Reconstruction from RGBD Scans (Thomas Funkhouser)</td>
          <td>4:30pm - 5:00pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>5:00pm - 5:30pm</td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="http://www.cvlibs.net"><img class="people-pic" style="float:left;margin-right:50px;" src="/cvpr2021workshop/static/img/people/andreas_geiger.png"></a>
    <p>
      <b><a href="http://www.cvlibs.net">Andreas Geiger</a></b> is professor at the University of Tübingen and group leader at the Max Planck Institute for Intelligent Systems. Prior to this, he was a visiting professor at ETH Zürich and a research scientist at MPI-IS. He studied at KIT, EPFL and MIT and received his PhD degree in 2013 from the KIT. His research interests are at the intersection of 3D reconstruction, motion estimation, scene understanding and sensory-motor control. He maintains the KITTI vision benchmark.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.sfu.ca/~furukawa/"><img class="people-pic" style="float:left;margin-right:50px;" src="/cvpr2021workshop/static/img/people/yasu_furukawa.png"></a>
    <p>
      <b><a href="https://www.cs.sfu.ca/~furukawa/">Yasutaka Furukawa</a></b> is an associate professor of Computing Science at Simon Fraser University. Prior to SFU, he was an assistant professor at Washington University in St. Louis. Before WUSTL, he was a software engineer at Google. Before Google, he was a post-doctoral research associate at University of Washington. He worked with Prof. Seitz and Prof. Curless at University of Washington, and Rick Szeliski at Facebook (was at Microsoft Research). He completed his Ph.D. under the supervision of Prof. Ponce at Computer Science Department of University of Illinois at Urbana-Champaign in May 2008.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.princeton.edu/~funk/"><img class="people-pic" style="float:left;margin-right:50px;" src="/cvpr2021workshop/static/img/people/tom_funkhouser.png"></a>
    <p>
      <b><a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a></b> is a senior research scientist at Google and the David
M. Siegel Professor of Computer Science, Emeritus, at Princeton University.  He received a PhD in computer science from UC Berkeley in
1993 and was a member of the technical staff at Bell Labs until 1997 before joining the faculty at Princeton.  For most of his career, he focused on research problems in computer graphics, including foundational work on 3D shape retrieval, analysis, and modeling.   His
most recent research has focused on 3D scene understanding in computer vision. He has published more than 100 research papers and received several awards, including the ACM SIGGRAPH Computer Graphics Achievement Award, ACM SIGGRAPH Academy, ACM Fellow, NSF Career Award, Emerson Electric, E. Lawrence Keyes Faculty Advancement Award, Google Faculty Research Awards, University Council Excellence in Teaching Awards, and a Sloan Fellowship.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://angeladai.github.io/">
      <img class="people-pic" src="/cvpr2021workshop/static/img/people/angela.png" />
    </a>
    <div class="people-name">
      <a href="https://angeladai.github.io/">Angela Dai</a>
      <h6>Technical University of Munich</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="/cvpr2021workshop/static/img/people/angel.png" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://msavva.github.io/">
      <img class="people-pic" src="/cvpr2021workshop/static/img/people/manolis.png" />
    </a>
    <div class="people-name">
      <a href="https://msavva.github.io/">Manolis Savva</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">
      <img class="people-pic" src="/cvpr2021workshop/static/img/people/matthias.png" />
    </a>
    <div class="people-name">
      <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a>
      <h6>Technical University of Munich</h6>
    </div>
  </div>
</div>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>